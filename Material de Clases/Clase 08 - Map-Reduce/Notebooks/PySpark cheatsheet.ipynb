{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-78-bW56HSAr",
        "outputId": "469b551f-c0f9-435c-a23d-8d0a2dad2946"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=2efb00431ca4fe1427f0cb54b33c12a96f9d144c758053cf5f7416c7c241cd58\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .getOrCreate()\n",
        "    \n",
        "sc = spark.sparkContext\n",
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "2EBflYEEHYin",
        "outputId": "a945ee83-ace5-4277-9f66-deb3450581e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://aa0fd0cbd44c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark cheatsheet\n",
        "\n",
        "La API de Spark tiene varias funciones, pero en este _notebook_ vamos a presentar las funciones más importantes de RDDs. Un RDD es un Resilient Distributed Dataset, que es la estructura básica de Apache Spark para trabajar con datos distribuidos. Estos datos se cargan (en general) desde un sistema de archivos distribuidos, por lo que podemos suponer que en un nodo de cómputo no se encuentran todos los datos, por lo que tenemos que usar estas funciones pensadas para trabajar en entornos distribuidos. Puedes ver la API completa [aquí](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis).\n"
      ],
      "metadata": {
        "id": "XIqoPNWAE1z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Map\n",
        "\n",
        "La función `map` retorna un nuevo RDD al que se le aplica una función en cada posición."
      ],
      "metadata": {
        "id": "BdilL5PPGuu0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JKCcG8FExlX",
        "outputId": "1813c78a-6c47-4f26-9549-f8873cdee7cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', 1), ('a', 1), ('c', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
        "rdd.map(lambda x: (x, 1)).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Reduce\n",
        "\n",
        "La función `reduce` pasa de un RDD a un único elemento"
      ],
      "metadata": {
        "id": "qhggQB4kIKD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "rdd.reduce(lambda x, y: x + y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9ACvDQRG1kW",
        "outputId": "11cf362d-9ce0-456d-a89e-e9bb6b73e24a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. ReduceByKey\n",
        "\n",
        "La función `reduceByKey` se encarga de formar un iterable para cada llave de un RDD. Luego se aplica la función reduce para cada iterable."
      ],
      "metadata": {
        "id": "E3qBgGajJ7RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "rdd.reduceByKey(lambda x, y: x + y).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wmC0I5MJoC6",
        "outputId": "1cfc6ec9-e478-4a77-b962-baa8586d4461"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', 1), ('a', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Count\n",
        "\n",
        "La función `count` cuenta los elementos en un RDD."
      ],
      "metadata": {
        "id": "2KJGJFoFM3tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.parallelize([2, 3, 4]).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHn6ivTcLwV8",
        "outputId": "0f7b8050-7238-482f-e511-55bc777d6482"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.parallelize([2, 3, 4, 4]).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ax-qybeNPOG",
        "outputId": "b258df69-d6e8-487e-c970-28bef3027417"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Distinct\n",
        "\n",
        "La función `distinct` retorna los elementos distintos de una colección."
      ],
      "metadata": {
        "id": "8CNl8mSZNRph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.parallelize([1, 1, 2, 3]).distinct().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAfhTc5VNQ0m",
        "outputId": "b7ebf306-e86b-40f5-b701-e58f8cd985ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 1, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.parallelize([(1, 1), (1, 2), (1, 1)]).distinct().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZDU10JvNbYH",
        "outputId": "6e43c3d3-a086-42c4-ad1e-0d7e92de09c6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 1), (1, 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Filter\n",
        "\n",
        "La función `filter`  aplica una función _booleana_, y deja solo los elementos que retornen `True` en la función."
      ],
      "metadata": {
        "id": "xzNd-XW0NmUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "rdd.filter(lambda x: x % 2 == 0).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UfkSjzQNkr7",
        "outputId": "dbbf44d9-a8a9-4376-a936-c9a92c84d631"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Join\n",
        "\n",
        "La función `join` nos permite hacer un _join_ según las llaves de dos RDD."
      ],
      "metadata": {
        "id": "7pUxljvXOGp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
        "rdd2 = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
        "\n",
        "rdd1.join(rdd2).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amrtjqYmN2Hw",
        "outputId": "41134397-b265-4ae3-dd02-c92de712ff50"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', (1, 2)), ('a', (1, 3))]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. LeftJoin\n",
        "\n",
        "La función `leftOuterJoin` hace un _left outer join_ entre dos RDDs. Al igual que en una BD relacional, el orden importa."
      ],
      "metadata": {
        "id": "HkMgPZmuO4P5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
        "rdd2 = sc.parallelize([(\"a\", 2)])\n",
        "\n",
        "rdd1.leftOuterJoin(rdd2).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_odMbmxbONtE",
        "outputId": "79dedf31-a2b9-4d85-f6b5-42aaca2b21ec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', (4, None)), ('a', (1, 2))]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Keys\n",
        "\n",
        "La función `keys` nos retorna las llaves en un RDD que representa pares key-value."
      ],
      "metadata": {
        "id": "fZ7O6hKHPWnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyInxscaPKMv",
        "outputId": "d4e0629f-38f4-4ac5-a005-a5a2024e7e47"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Values\n",
        "\n",
        "La función `values` nos retorna los valores en un RDD que representa pares key-value."
      ],
      "metadata": {
        "id": "ABM-xiAcPfnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([(1, 2), (3, 4)]).values()\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7_pUMTUPcI9",
        "outputId": "77b7c861-b279-49d7-ddc5-06a0e9acc4b7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. FlatMap\n",
        "\n",
        "La función `flatMap` permite ejecutar la función `map` extrayendo los elementos de los iterables que genera."
      ],
      "metadata": {
        "id": "mxtyOUYWSPHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([2, 3, 4])\n",
        "rdd.flatMap(lambda x: range(1, x)).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn459lBePl_T",
        "outputId": "76da8e3a-ef04-40da-ea49-8a27cf3f24d5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 2, 1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora veamos qué retorna la función `map` común."
      ],
      "metadata": {
        "id": "bgRKyFvXSYtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([2, 3, 4])\n",
        "rdd.map(lambda x: range(1, x)).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghTsYJvtSYBP",
        "outputId": "6b21d0c8-c587-4a77-8654-43947b9f61ba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[range(1, 2), range(1, 3), range(1, 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otro ejemplo de `flatMap` es este."
      ],
      "metadata": {
        "id": "vtGz9UPXSokL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([2, 3, 4])\n",
        "rdd.flatMap(lambda x: [(x, x), (x, x)]).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7gkdSizSh19",
        "outputId": "73c8b650-f848-4b11-d656-b479e7939fd4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Zip\n",
        "\n",
        "La función `zip` permite juntar dos `RDD` en uno solo."
      ],
      "metadata": {
        "id": "3PRK2fN0S3C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = sc.parallelize([1, 2, 3, 4, 5])\n",
        "rdd2 = sc.parallelize([6, 7, 8, 9, 10])\n",
        "\n",
        "rdd1.zip(rdd2).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDNiCMzLStr7",
        "outputId": "f3dbd98f-fe84-49dc-d385-88d703c83b34"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 6), (2, 7), (3, 8), (4, 9), (5, 10)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. AggregateByKey\n",
        "\n",
        "La función AggregateByKey es similar a la función `reduceByKey`, pero permite hacer operaciones más complejas. La idea es agregar en cada partición local, partiendo de un valor inicial comunmente conocido como el neutro. Luego se agregan los resultados locales con una función que combina los resultados por partición."
      ],
      "metadata": {
        "id": "f5DmTXaUUbrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
        "\n",
        "# Para cada key, vamos ejecutando la función de agregación entre pares\n",
        "# partiendo desde el elemento inicial\n",
        "\n",
        "# Función conmutativa, no podemos asumir orden\n",
        "seqFunc = (lambda x, y: (x[0] + y, x[1] + 1))\n",
        "combFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
        "rdd.aggregateByKey(\n",
        "  (0, 0), # Elemento inicial\n",
        "  seqFunc, # Función de agregación local\n",
        "  combFunc # Función para combinar resultados locales\n",
        ").collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCmUWwPaZVRG",
        "outputId": "f1e97dd0-34be-4bbb-d566-eb27d37fd5fa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', (3, 2)), ('b', (1, 1))]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo que estamos haciendo para cada llave es calcular la suma de todos sus elementos, junto al número de elementos. En este ejemplo valor inicial es `(0, 0)`. Así que analicemos el caso para la llave `a`. Supongamos que tenemos el iterable en el orden `[2, 1]`, la función de agregación hace lo siguiente:\n",
        "\n",
        "1. Primero generamos la tupla `(x[0] + y, x[1] + 1)`, donde `x` es la tupla con el valor inicial e `y` el primer valor del iterable. Por lo tanto hacemos `(0 + 2, 0 + 1)`.\n",
        "2. Luego `x` tiene el valor del acumulado hasta ahora. Así que la siguiente operación es: `(2 + 1, 1 + 1)`.\n",
        "\n",
        "Ahora, si no todos los valores de `a` están en el mismo nodo de cómputo, estos se comunicarán después de la agregación local. La idea de esta forma de computar cosas es que minimizamos los cómputos entre servidores."
      ],
      "metadata": {
        "id": "mFFhZYiyaqyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. MapValues\n",
        "\n",
        "Ejecutamos un `map` para todos los _values_ de un RDD que almacena pares key-value."
      ],
      "metadata": {
        "id": "si0TFG_DeW7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
        "rdd.mapValues(lambda x: len(x)).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv-fbxWAedj_",
        "outputId": "522799af-008d-458d-dbca-b1b2d2443e88"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 3), ('b', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ojo**. Si el valor es un iterable, no aplicamos la función a cada elemento del iterable, sino al iterable entero. Esto es porque estamos mapeando todos los _values_ del RDD. Tampoco se agrupa por cada llave."
      ],
      "metadata": {
        "id": "kih1V19deo_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"]), (\"a\", [\"kiwi\"])])\n",
        "rdd.mapValues(lambda x: len(x)).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXUtYCVge11s",
        "outputId": "0a2e6180-3cc8-434b-dfe1-e95399a8a3ff"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 3), ('b', 1), ('a', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. GroupByKey\n",
        "\n",
        "La función `groupByKey` permite juntar todos los elementos para cada llave. **Ojo**. Se debe privilegiar el uso de `reduceByKey` o `aggregateByKey` porque tienen mucho mejor desempeño."
      ],
      "metadata": {
        "id": "I6hqvy35dBBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
        "rdd.groupByKey().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMXD3apGdmKg",
        "outputId": "42180df8-794d-4c27-c6a3-9e968658c708"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', <pyspark.resultiterable.ResultIterable at 0x7fd492b55510>),\n",
              " ('a', <pyspark.resultiterable.ResultIterable at 0x7fd492b55a80>)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para poder hacer algo con los elementos podemos hacer lo siguiente."
      ],
      "metadata": {
        "id": "X2dBIqg5d5il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
        "rdd.groupByKey().mapValues(list).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oagxgQJHd8k9",
        "outputId": "a4749435-8364-46c0-9049-7758aa014d83"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', [1]), ('a', [1, 2])]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos otro ejemplo."
      ],
      "metadata": {
        "id": "EqbLWMGwfGwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
        "rdd.groupByKey().mapValues(len).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sO9YTS93fGhr",
        "outputId": "5a1f6181-a95b-422e-f772-98ce2f577ef0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', 1), ('a', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus: la función Broadcast\n",
        "\n",
        "La función `broadcast` no es de la API de RDD, sino que se asocia al _Spark Context_. Permite crear una variable accesible para todos los nodos de cómputo."
      ],
      "metadata": {
        "id": "-0LQNsCYTio4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {1: 10001, 2: 10002}\n",
        "bc = sc.broadcast(mapping)\n",
        "\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "rdd2 = rdd.map(lambda x: bc.value[x] if x in bc.value else -1)\n",
        "\n",
        "rdd2.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9Q8dPxVTMVb",
        "outputId": "6fa833f1-147b-46f9-8543-8b3d88cd9621"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10001, 10002, -1, -1, -1]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos que la idea es hacer _broadcasting_ de variables pequeñas. También podemos destruirlas."
      ],
      "metadata": {
        "id": "pdP5dY-rUTwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bc.destroy()"
      ],
      "metadata": {
        "id": "36QwP-rxUDxo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iws8M9PXUakT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
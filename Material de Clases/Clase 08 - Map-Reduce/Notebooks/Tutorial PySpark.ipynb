{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9yZ2ZiRKiZB",
        "outputId": "90acbb80-99aa-46b1-e185-28a53a8c1ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=0196b7ad75b34f70cac8a64df0a2bd6f44a90454dccad9a591c2618c0febfd4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usando Spark en Python\n",
        "\n",
        "En este notebook vamos a aprender a desarrollar en Spark con Python, para eso vamos a usar la librería `pyspark`. Al ejecutar la celda anterior, estamos instalando `pyspark` en Google Colab. Ahora estamos listos para dar nuestros primeros pasos con esta librería.\n",
        "\n",
        "**Importante**: recuerda que comunmente usamos `pyspark` junto con _dataframes_ o con SQL, pero la idea de este notebook es entender las operaciones de más bajo nivel que nos ofrece Apache Spark.\n",
        "\n",
        "Vamos a partir cargando la librería e inicializando la variable `sc`, que es el contexto de Spark. Cuando estamos corriendo Spark en un cluster, este contexto apunta al cluster de computadores, y finalmente el código que se corre en un entorno local (single node) es igual al que se corre en un entorno distribuido."
      ],
      "metadata": {
        "id": "LDtbxhLbK0sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .getOrCreate()\n",
        "    \n",
        "sc = spark.sparkContext\n",
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "DEEGeZ2gL2va",
        "outputId": "d64f6449-928d-47d5-8bcf-2ff71ae622db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://59512bc4521b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Primera parte: el hello world de la computación distribuida\n",
        "\n",
        "Lo primero que vamos a hacer es ejecutar el _hello world_ en computación distribuida, que es contar el número de palabras por palabra de un archivo. Para esto, cargamos el archivo `demo_file_spark.txt` que está junto a este _notebook_."
      ],
      "metadata": {
        "id": "6SBh8sXPL0Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = sc.textFile(\"demo_file_spark.txt\")\n",
        "text_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4C-VyzUKwkR",
        "outputId": "c50d6aaf-c489-42c5-990f-b4136a9944bd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "demo_file_spark.txt MapPartitionsRDD[27] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variable `text_file` es de tipo `RDD`: Resilient Distributed Dataset. Este es un archivo que está potencialmente distribuido en varios nodos (por ejemplo, en un HDFS). No vemos nada al cargar la variable `demo_file_spark.txt` porque está distribuido, así que para cargarlo en un nodo e imprimirlo debemos usar la función `collect()`."
      ],
      "metadata": {
        "id": "MHruyvo_Mvk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5IcITflMvVH",
        "outputId": "821a686b-1a1e-451c-fb3f-0cda924e12b2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Deer Bear River', 'Car Car River', 'Deer Car Bear']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vemos, cada línea del archivo es parte de una lista. Al trabajar en un cluster, si no hubiesemos ejecutado la función `collect()`, cada elemento de la lista podría estar en cualquiera de los nodos del cluster. \n",
        "\n",
        "Ahora vamos a ejecutar nuestro primer `map()`. Esta función tomará cada línea y hará un `split()` por espacio."
      ],
      "metadata": {
        "id": "WK74RnYcNhC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file.map(lambda line: line.split(\" \")).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFp70gClMrBM",
        "outputId": "cdc0eb5b-256b-414b-e490-f113a6b7e757"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Deer', 'Bear', 'River'], ['Car', 'Car', 'River'], ['Deer', 'Car', 'Bear']]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vemos, es lo que esperamos de una función `map()`. Notemos que si no hacemos `collect()` no podríamos ver los resultados, porque el `map()` se ejecuta de forma distribuida. \n",
        "\n",
        "Ahora vamos a hacer un `flatMap()`, que \"aplana\" los resultados de la función `map()`. Esto es, si la función `map()` genera una colección de elementos de largo mayor que 1, entonces los elementos \"salen\" de la colección."
      ],
      "metadata": {
        "id": "vzYo0rdhOUp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file.flatMap(lambda line: line.split(\" \")).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDwyqBF3OScn",
        "outputId": "aa08c481-0bc2-483f-8cd7-bb6ece33c238"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Deer', 'Bear', 'River', 'Car', 'Car', 'River', 'Deer', 'Car', 'Bear']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vemos, en vez de tener una lista de listas, nos quedamos con una lista de palabras. Ahora, para continuar con el conteo de la aparición de cada palabra podemos hacer que cada palabra se emita junto al número 1, para después sumar cuantos 1s hay por palabra."
      ],
      "metadata": {
        "id": "D30rX3qPQG6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "  .map(lambda word: (word, 1)).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BO1vGqjQF7E",
        "outputId": "72e86255-c34d-4fb4-cb45-be6c83ef727b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Deer', 1),\n",
              " ('Bear', 1),\n",
              " ('River', 1),\n",
              " ('Car', 1),\n",
              " ('Car', 1),\n",
              " ('River', 1),\n",
              " ('Deer', 1),\n",
              " ('Car', 1),\n",
              " ('Bear', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y el resultado anterior lo podemos reducir para cada una de sus llaves."
      ],
      "metadata": {
        "id": "KoHcznbHQi2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "  .map(lambda word: (word, 1)) \\\n",
        "  .reduceByKey(lambda a, b: a + b)\n",
        "  \n",
        "counts.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ETY_2bIQiiR",
        "outputId": "986a30fb-2b88-4cd5-dcfc-cd0e428ea892"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Bear', 2), ('Deer', 2), ('River', 2), ('Car', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso, para cada par emitido por la instrucción `.map(lambda word: (word, 1))`, lo que hacemos es considerar como llave el primer elemento de la tupla, y como valor el segundo. Así, al aplicar `reduceByKey`, para cada llave, armamos una colección con todos los elementos que le corresponden a esa llave (que serían solamente 1s). En el caso del archivo anterior sería algo así:\n",
        "\n",
        "```Bash\n",
        "\"Deer\": [1, 1]\n",
        "\"Bear\": [1, 1]\n",
        "\"River\" [1, 1]\n",
        "\"Car\": [1, 1, 1]\n",
        "```\n",
        "\n",
        "Luego, aplicamos la función `lambda a, b: a + b` a cada una de las listas, por cada llave."
      ],
      "metadata": {
        "id": "UkiPw6eEQsHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leyendo un archivo con logs\n",
        "\n",
        "Un caso de uso típico para usar Apache Spark es la lectura de archivos de logs extensos. En caso de tener registros amplios, es natural montar estos archivos en un sistema de archivos distribuido para analizarlos. Ahora vamos a ver un ejemplo cargando el archivo `logs.txt` que encontrarás junto a este _notebook_."
      ],
      "metadata": {
        "id": "Uac-vGhBTEnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = sc.textFile(\"logs.txt\")\n",
        "\n",
        "# Toma una muestra aleatoria de 10 elementos sin reemplazo\n",
        "text_file.takeSample(False, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_V2YoITQcpL",
        "outputId": "4087080f-b5b1-434c-b178-2c5ce9d5c6b1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Operación 151 ejecutada en la base de datos',\n",
              " 'Operación 543 ejecutada en la aplicación web',\n",
              " 'Operación 946 ejecutada en la base de datos',\n",
              " 'Operación 274 ejecutada en la aplicación web',\n",
              " 'Operación 815 ejecutada en la aplicación web',\n",
              " 'Operación 434 ejecutada en la base de datos',\n",
              " 'Operación 466 ejecutada en la aplicación web',\n",
              " 'Operación 897 ejecutada en la base de datos',\n",
              " 'Operación 334 ejecutada en la aplicación web',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 70']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para no ver el archivo entero, estamos obteniendo un sample aleatorio sin reemplazo de 10 elementos. Ahora que entendemos cómo se ve el archivo, intentemos encontrar todos los registros que hablan de errores. Para esto vamos a usar la función `filter()`, que retorna un `RDD` nuevo que solo contiene los elementos que cumplen con cierta condición."
      ],
      "metadata": {
        "id": "FKTu-SUAU9H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs_filter = text_file.filter(lambda line: \"Error\" in line)\n",
        "logs_filter.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOMxId2UUzOV",
        "outputId": "7f5474a0-2e0f-4cb3-9651-519de5cbbfb4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Error de la base de datos en la operación 10',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 14',\n",
              " 'Error de la base de datos en la operación 15',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 17',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 27',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 28',\n",
              " 'Error de la base de datos en la operación 31',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 35',\n",
              " 'Error de la base de datos en la operación 37',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 39',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 40',\n",
              " 'Error de la base de datos en la operación 41',\n",
              " 'Error de la base de datos en la operación 42',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 43',\n",
              " 'Error de la base de datos en la operación 45',\n",
              " 'Error de la base de datos en la operación 46',\n",
              " 'Error de la base de datos en la operación 49',\n",
              " 'Error de la base de datos en la operación 50',\n",
              " 'Error de la base de datos en la operación 53',\n",
              " 'Error de la base de datos en la operación 56',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 60',\n",
              " 'Error de la base de datos en la operación 62',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 65',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 67',\n",
              " 'Error de la base de datos en la operación 68',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 70',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 78',\n",
              " 'Error de la base de datos en la operación 83',\n",
              " 'Error de la base de datos en la operación 86',\n",
              " 'Error de la base de datos en la operación 91',\n",
              " 'Error de la base de datos en la operación 101',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 102',\n",
              " 'Error de la base de datos en la operación 109',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 115',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 118',\n",
              " 'Error de la base de datos en la operación 126',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 133',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 140',\n",
              " 'Error de la base de datos en la operación 143',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 159',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 165',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 166',\n",
              " 'Error de la base de datos en la operación 185',\n",
              " 'Error de la base de datos en la operación 188',\n",
              " 'Error de la base de datos en la operación 195',\n",
              " 'Error de la base de datos en la operación 210',\n",
              " 'Error de la base de datos en la operación 219',\n",
              " 'Error de la base de datos en la operación 222',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 227',\n",
              " 'Error de la base de datos en la operación 228',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 238',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 239',\n",
              " 'Error de la base de datos en la operación 241',\n",
              " 'Error de la base de datos en la operación 243',\n",
              " 'Error de la base de datos en la operación 246',\n",
              " 'Error de la base de datos en la operación 247',\n",
              " 'Error de la base de datos en la operación 253',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 261',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 267',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 268',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 276',\n",
              " 'Error de la base de datos en la operación 280',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 282',\n",
              " 'Error de la base de datos en la operación 287',\n",
              " 'Error de la base de datos en la operación 290',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 292',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 297',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 301',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 302',\n",
              " 'Error de la base de datos en la operación 307',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 308',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 313',\n",
              " 'Error de la base de datos en la operación 315',\n",
              " 'Error de la base de datos en la operación 322',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 324',\n",
              " 'Error de la base de datos en la operación 338',\n",
              " 'Error de la base de datos en la operación 339',\n",
              " 'Error de la base de datos en la operación 340',\n",
              " 'Error de la base de datos en la operación 341',\n",
              " 'Error de la base de datos en la operación 345',\n",
              " 'Error de la base de datos en la operación 347',\n",
              " 'Error de la base de datos en la operación 349',\n",
              " 'Error de la base de datos en la operación 350',\n",
              " 'Error de la base de datos en la operación 354',\n",
              " 'Error de la base de datos en la operación 358',\n",
              " 'Error de la base de datos en la operación 360',\n",
              " 'Error de la base de datos en la operación 364',\n",
              " 'Error de la base de datos en la operación 368',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 369',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 374',\n",
              " 'Error de la base de datos en la operación 378',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 380',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 405',\n",
              " 'Error de la base de datos en la operación 406',\n",
              " 'Error de la base de datos en la operación 411',\n",
              " 'Error de la base de datos en la operación 422',\n",
              " 'Error de la base de datos en la operación 428',\n",
              " 'Error de la base de datos en la operación 429',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 442',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 445',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 449',\n",
              " 'Error de la base de datos en la operación 452',\n",
              " 'Error de la base de datos en la operación 458',\n",
              " 'Error de la base de datos en la operación 465',\n",
              " 'Error de la base de datos en la operación 471',\n",
              " 'Error de la base de datos en la operación 474',\n",
              " 'Error de la base de datos en la operación 476',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 484',\n",
              " 'Error de la base de datos en la operación 485',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 488',\n",
              " 'Error de la base de datos en la operación 496',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 504',\n",
              " 'Error de la base de datos en la operación 505',\n",
              " 'Error de la base de datos en la operación 506',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 507',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 511',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 512',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 513',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 516',\n",
              " 'Error de la base de datos en la operación 523',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 526',\n",
              " 'Error de la base de datos en la operación 527',\n",
              " 'Error de la base de datos en la operación 542',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 554',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 561',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 565',\n",
              " 'Error de la base de datos en la operación 571',\n",
              " 'Error de la base de datos en la operación 578',\n",
              " 'Error de la base de datos en la operación 590',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 593',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 610',\n",
              " 'Error de la base de datos en la operación 613',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 615',\n",
              " 'Error de la base de datos en la operación 624',\n",
              " 'Error de la base de datos en la operación 627',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 629',\n",
              " 'Error de la base de datos en la operación 632',\n",
              " 'Error de la base de datos en la operación 639',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 651',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 656',\n",
              " 'Error de la base de datos en la operación 660',\n",
              " 'Error de la base de datos en la operación 662',\n",
              " 'Error de la base de datos en la operación 664',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 666',\n",
              " 'Error de la base de datos en la operación 667',\n",
              " 'Error de la base de datos en la operación 678',\n",
              " 'Error de la base de datos en la operación 682',\n",
              " 'Error de la base de datos en la operación 684',\n",
              " 'Error de la base de datos en la operación 689',\n",
              " 'Error de la base de datos en la operación 703',\n",
              " 'Error de la base de datos en la operación 704',\n",
              " 'Error de la base de datos en la operación 709',\n",
              " 'Error de la base de datos en la operación 710',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 717',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 719',\n",
              " 'Error de la base de datos en la operación 722',\n",
              " 'Error de la base de datos en la operación 723',\n",
              " 'Error de la base de datos en la operación 734',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 736',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 742',\n",
              " 'Error de la base de datos en la operación 750',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 751',\n",
              " 'Error de la base de datos en la operación 754',\n",
              " 'Error de la base de datos en la operación 760',\n",
              " 'Error de la base de datos en la operación 769',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 771',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 779',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 782',\n",
              " 'Error de la base de datos en la operación 784',\n",
              " 'Error de la base de datos en la operación 790',\n",
              " 'Error de la base de datos en la operación 805',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 806',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 807',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 810',\n",
              " 'Error de la base de datos en la operación 812',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 813',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 817',\n",
              " 'Error de la base de datos en la operación 820',\n",
              " 'Error de la base de datos en la operación 825',\n",
              " 'Error de la base de datos en la operación 827',\n",
              " 'Error de la base de datos en la operación 831',\n",
              " 'Error de la base de datos en la operación 832',\n",
              " 'Error de la base de datos en la operación 835',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 839',\n",
              " 'Error de la base de datos en la operación 845',\n",
              " 'Error de la base de datos en la operación 863',\n",
              " 'Error de la base de datos en la operación 864',\n",
              " 'Error de la base de datos en la operación 865',\n",
              " 'Error de la base de datos en la operación 867',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 879',\n",
              " 'Error de la base de datos en la operación 888',\n",
              " 'Error de la base de datos en la operación 896',\n",
              " 'Error de la base de datos en la operación 902',\n",
              " 'Error de la base de datos en la operación 903',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 908',\n",
              " 'Error de la base de datos en la operación 909',\n",
              " 'Error de la base de datos en la operación 911',\n",
              " 'Error de la base de datos en la operación 912',\n",
              " 'Error de la base de datos en la operación 923',\n",
              " 'Error de la base de datos en la operación 926',\n",
              " 'Error de la base de datos en la operación 927',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 928',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 931',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 933',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 934',\n",
              " 'Error de la base de datos en la operación 935',\n",
              " 'Error de la base de datos en la operación 940',\n",
              " 'Error de la base de datos en la operación 942',\n",
              " 'Error de la base de datos en la operación 947',\n",
              " 'Error de la base de datos en la operación 949',\n",
              " 'Error de la base de datos en la operación 951',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 955',\n",
              " 'Error de la aplicación web de tipo 400 en la operación 971',\n",
              " 'Error de la base de datos en la operación 972',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 973',\n",
              " 'Error de la base de datos en la operación 983',\n",
              " 'Error de la base de datos en la operación 988',\n",
              " 'Error de la aplicación web de tipo 500 en la operación 990']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora puedes probar tú haciendo otra clase de filtros sobre el archivo."
      ],
      "metadata": {
        "id": "XvCxHqNIVXM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargando un archivo como DataFrame\n",
        "\n",
        "Si bien no es el objetivo de este _notebook_, vamos a mostrar cómo cargar el archivo anterior en un `DataFrame` para conseguir el mismo filtro."
      ],
      "metadata": {
        "id": "RlIQqmBoVdYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "textFile = sc.textFile(\"logs.txt\")\n",
        "\n",
        "# Creamos un DataFrame con una única columna llamada line\n",
        "df = textFile.map(lambda r: Row(r)).toDF([\"line\"])\n",
        "\n",
        "# Filtramos por las que contengan el texto Error\n",
        "errors = df.filter(col(\"line\").like(\"%Error%\"))\n",
        "\n",
        "# Contamos el número de errores\n",
        "errors.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJIGPcnrVIah",
        "outputId": "1917c1ca-ea3f-4752-a83d-f2f18f2c12bf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos hacer operaciones más complejas y cargar DataFrames de más columnas, pero es importante recordar que la implementación de Spark de un DataFrame es un subconjunto de la que encontramos en Pandas, porque no todas las funciones pueden ser implementadas en el contexto de computación distribuida."
      ],
      "metadata": {
        "id": "ysXqLP8vYA4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joins en pyspark\n",
        "\n",
        "Ahora probemos cómo hacer joins en `pyspark`, para esto vamos a partir con un ejemplo muy sencillo."
      ],
      "metadata": {
        "id": "hdOXzg2xYYpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de tabla 1\n",
        "t1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
        "\n",
        "# Ejemplo de tabla 2\n",
        "t2 = sc.parallelize([(\"a\", 2), (\"a\", 3)])"
      ],
      "metadata": {
        "id": "MjvhoRfKV-wd"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estamos instanciando dos listas de tuplas. Esto simula dos tablas."
      ],
      "metadata": {
        "id": "iy_aZoIbYus-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de7FSPinYuD5",
        "outputId": "2450460e-7692-486b-9967-b3b3cdfb5fce"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 1), ('b', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t2.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TBQdvjnY44V",
        "outputId": "2c79a610-7d2c-478a-f8c3-25b5d623bef4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 2), ('a', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos que en Spark, cuando tenemos una tupla, el primer elemento actua de llave y el segundo de valor. Entonces al usar el comando `join()`, vamos a hacer un join por las llaves, juntando los valores correspondientes."
      ],
      "metadata": {
        "id": "tN-Cp4I-Y7AN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1.join(t2).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkZVkmv6Y6Hs",
        "outputId": "f75d5be0-26dd-43f8-ba17-be3fde874bd6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', (1, 2)), ('a', (1, 3))]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vemos, es el resultado que esperamos. Si queremos simular tablas de más atributos, podemos tener un \"value\" que representa una tupla, para guardar más elementos."
      ],
      "metadata": {
        "id": "OPf50qNtZGZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = sc.parallelize([(\"a\", (1, 3)), (\"b\", (4, 5))])\n",
        "t2 = sc.parallelize([(\"a\", (2, 6)), (\"a\", (3, 7))])\n",
        "\n",
        "t1.join(t2).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89d0_ZRWZD-y",
        "outputId": "957d9540-7769-4897-8f52-3a89a70b794c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', ((1, 3), (2, 6))), ('a', ((1, 3), (3, 7)))]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora vamos a ver cómo cargar líneas de un archivo que representan tablas en `pyspark`. Para esto vamos a cargar el archivo `example_join.txt` que está junto a este _notebook_ y se ve de la siguiente forma:\n",
        "\n",
        "```\n",
        "T1,a,1\n",
        "T1,b,4\n",
        "T2,a,2\n",
        "T2,a,3\n",
        "```\n",
        "\n",
        "En donde señalamos qué línea pertenece a qué tabla. Para cargar las dos tablas en `pyspark` podemos hacer lo siguiente."
      ],
      "metadata": {
        "id": "EN3jRi1_Zuiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = sc.textFile(\"example_join.txt\")\n",
        "text_file.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7HyMVwoZqDB",
        "outputId": "e650e676-9c26-4ffd-d0ab-282a2b379058"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T1,a,1', 'T1,b,4', 'T2,a,2', 'T2,a,3']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos funciones para filtrar los elementos de cada tabla."
      ],
      "metadata": {
        "id": "ioYw0GkTaWYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_table_t1(line):\n",
        "    return line.split(\",\")[0] == \"T1\"\n",
        "\n",
        "def get_table_t2(line):\n",
        "    return line.split(\",\")[0] == \"T2\"\n",
        "\n",
        "text_file.filter(get_table_t1).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEfBoIQkaOlq",
        "outputId": "b937c1c7-c0f6-4c5e-8fb1-430f6d89e560"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T1,a,1', 'T1,b,4']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_t1_raw = text_file.filter(get_table_t1)\n",
        "table_t2_raw = text_file.filter(get_table_t2)"
      ],
      "metadata": {
        "id": "bPpCrwG1abHx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora pasamos las tuplas de cada tabla al formato que sabemos que necesita `pyspark`."
      ],
      "metadata": {
        "id": "N6qYb8muag0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_tuples(line):\n",
        "    row = line.split(\",\")\n",
        "    return (row[1], int(row[2]))\n",
        "\n",
        "table_t1_raw.map(convert_to_tuples).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpzYIeB4aeKy",
        "outputId": "aa6658f9-21bb-4c5f-ceaf-8cbf7d813905"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 1), ('b', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_t1 = table_t1_raw.map(convert_to_tuples)\n",
        "table_t2 = table_t2_raw.map(convert_to_tuples)"
      ],
      "metadata": {
        "id": "1_7X6sQiakvP"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y hacemos el join."
      ],
      "metadata": {
        "id": "lvgcOtolaogn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_t1.join(table_t2).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jESbWJ8gam3c",
        "outputId": "5f3221b7-0fb4-42ca-80c8-5d7367ac48f4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', (1, 2)), ('a', (1, 3))]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardando los resultados\n",
        "\n",
        "Recordemos que podemos guardar resultados como archivos de texto. Esto es especialmente útil si el resultado es grande y vamos a necesitar guardarlo en un sistema de archivos distribuido."
      ],
      "metadata": {
        "id": "ianlJG0Yav3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_t1.join(table_t2).saveAsTextFile(\"join_result_folder_v2\")"
      ],
      "metadata": {
        "id": "Sv6chPBwapqq"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resultado es una carpeta con cada una de las particiones, que debemos juntar para obtener el resultado completo."
      ],
      "metadata": {
        "id": "OZtEU9BmbA3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Broadcasting\n",
        "\n",
        "Cuando tenemos un set de datos pequeño que necesitamos que haga join con una tabla grande, es preferible enviar la tabla pequeña a cada uno de los nodos del cluster. Este proceso se llama broadcasting. Aquí tenemos un ejemplo."
      ],
      "metadata": {
        "id": "QuFal-dkbJkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = {\n",
        "        1 : 'Red',\n",
        "        2 : 'Blue',\n",
        "        3 : 'Green',\n",
        "        4 : 'Yellow',\n",
        "    }\n",
        "t1_broadcast = sc.broadcast(t1)\n",
        "\n",
        "data = [\n",
        "    [1, 1.23],\n",
        "    [2, 2.34],\n",
        "    [3, 3.45],\n",
        "    [4, 4.23],\n",
        "    [1, 32.2],\n",
        "    [2, 22.2],\n",
        "    [4, 222.3]\n",
        "]\n",
        "\n",
        "t2 = sc.parallelize(data)\n",
        "\n",
        "t2.map(lambda x: [t1_broadcast.value[x[0]], x[1]]).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOdIni6na3oG",
        "outputId": "9a17ed46-539d-45fe-b21e-b1e9a2b1884e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Red', 1.23],\n",
              " ['Blue', 2.34],\n",
              " ['Green', 3.45],\n",
              " ['Yellow', 4.23],\n",
              " ['Red', 32.2],\n",
              " ['Blue', 22.2],\n",
              " ['Yellow', 222.3]]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lqi7__H6jSkj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}